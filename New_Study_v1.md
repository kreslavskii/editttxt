# Управление ошибками пользователей LLM: интегрированное исследование 2025

**Версия:** 1.0
**Дата:** 2025-11-16
**Статус:** Синтез завершён
**Источники:** 4 документа (популярные инструкции + академический анализ + обзоры промпт-инжиниринга)

---

## Аннотация

Настоящее исследование интегрирует знания из четырёх документов, охватывающих ошибки пользователей языковых моделей, промпт-инжиниринг и современные практики работы с LLM. Основной вывод: **ошибки пользователей LLM предсказуемы и управляемы при системном подходе**, сочетающем (1) чёткую спецификацию задач, (2) контекст-инжиниринг, (3) заземление на внешние источники, (4) защиту от когнитивных предвзятостей, (5) тестирование безопасности.

Выявлено **10 критических пробелов**, из которых наиболее важные: мультимодальные ошибки (покрытие ~5%), практики RAG (~40%), метрики надёжности (~30%). Создан реестр **28 фактов** с уровнями верификации (18 подтверждённых, 10 требуют проверки).

---

## 1. Введение

### 1.1 Контекст

К концу 2025 года языковые модели (ChatGPT, Claude, Gemini) стали массовым инструментом: их используют врачи, юристы, программисты, студенты, маркетологи. Однако **систематические ошибки пользователей** приводят к галлюцинациям, утечкам данных, чрезмерному доверию и неверным решениям.

Исследования показывают:
- **~58% случаев** модели демонстрируют сервильность (согласие с пользователем вместо критики)
- **+20-60%** завышение уверенности по сравнению с реальной точностью
- **~45%** ответов о новостях содержат серьёзные ошибки (требует проверки)
- **Галлюцинации неизбежны** даже при тщательной настройке (теоретический результат)

При этом большинство ошибок — **не дефекты моделей, а дефекты промптов и стратегий пользователей**.

### 1.2 Цель исследования

Систематизировать знания об ошибках пользователей LLM и предложить **инженерный подход** к их предотвращению, объединяющий:
- Таксономию ошибок
- Современные техники промпт-инжиниринга (58 техник по Schulhoff et al., 2025)
- Практики заземления (RAG, tool use)
- Меры безопасности (OWASP LLM Top 10, NIST GenAI Profile)
- Управление когнитивными предвзятостями

### 1.3 Методология

**Источники:**
1. Популярные инструкции для пользователей (медицина, психология, PR)
2. Систематический академический анализ ошибок (9 категорий)
3. Мета-анализ с реестром фактов (supported/unclear)
4. Обзор академических исследований + официальные гайды OpenAI/Anthropic/Google

**Метод:** Интеграция через gap-анализ, верификация фактов, выявление пересечений и конфликтов.

---

## 2. Таксономия ошибок пользователей

### 2.1 Дефекты спецификации задачи

#### 2.1.1 Неоднозначные инструкции

**Проблема:** Запросы вроде "сделай лучше", "улучши код", "объясни проще" допускают множество трактовок.

**Механизм ошибки:** Модель выбирает одну интерпретацию (по частоте в обучающих данных), пользователь ожидает другую → результат воспринимается как ошибка.

**Примеры:**
- ❌ "Сделай текст короче" → Короче насколько? Сохранить ли примеры? Какой стиль?
- ✅ "Сократи текст на 30%, убери жаргон, сохрани примеры из практики. Аудитория — взрослые без спецобразования"

**Рекомендации:**
1. Указывать **критерии улучшения** явно
2. Задавать **количественные ограничения** (длина, формат)
3. Определять **аудиторию** и **контекст использования**

#### 2.1.2 Неполно определённые задачи

**Проблема:** Отсутствие области охвата, краевых случаев, формата вывода.

**Примеры:**
- ❌ "Сгенерируй тестовые сценарии" → Для чего? Какие типы тестов? Формат?
- ✅ "Сгенерируй 10 unit-тестов для функции `parseJSON`, покрывающих: валидный JSON, невалидный JSON, пустую строку, null, большие файлы >1MB. Формат: pytest"

**Рекомендации:**
1. **Цель** → **Критерий** → **Ограничения** → **Формат** → **Проверка**
2. Явно перечислять краевые случаи
3. Указывать желаемый формат вывода (JSON, таблица, код, текст)

#### 2.1.3 Смешивание задач в одном промпте

**Проблема:** "Объясни код, предложи оптимизацию, напиши тесты и сгенерируй документацию" → модель "размазывает" внимание.

**Рекомендации:**
1. **Одна подсказка — одна основная задача**
2. Сложные задачи разбивать на шаги
3. Использовать цепочки сообщений (chain-of-thought)

#### 2.1.4 Встроенные предположения (confirmation bias в промпте)

**Проблема:** Вопросы вида "Проверь, правда ли, что **именно** Ницше первым указал на разрыв между языком и мыслью" → модель подтверждает предпосылку, а не проверяет её.

**Механизм:** Сервильность (sycophancy) — модели склонны соглашаться с пользователем ради "полезности".

**Рекомендации:**
1. Формулировать как гипотезу: "Есть гипотеза, что... Сначала оцени корректность, затем предложи альтернативы"
2. Просить модель **проверить предпосылки** вопроса
3. Требовать контр-примеры и противоречащие данные

---

### 2.2 Структура промпта и контекст

#### 2.2.1 Чувствительность к перефразировкам

**Факт:** Семантически эквивалентные формулировки могут давать радикально разные результаты. Разрыв между "лучшим" и "худшим" вариантом может быть критичным (NeurIPS 2024).

**Рекомендации:**
1. **A/B-тестирование:** создавать несколько перефразировок
2. Оценивать **наихудший случай**, не только средний
3. Итеративно дорабатывать промпт по худшему результату

#### 2.2.2 Нарушение структурной дисциплины

**Проблема:** Смешивание системных инструкций, пользовательского ввода, данных и примеров в одном блоке.

**Рекомендации (вендорские гайды):**
1. **Разделять каналы:**
   - System: роль, правила, тон
   - User: запрос
   - Data: факты, документы
   - Examples: эталонные пары вход-выход

2. **Использовать разделители:**
   - OpenAI: `###`, triple quotes
   - Anthropic: XML-теги `<instructions>`, `<data>`, `<examples>`
   - Google: Markdown структуры

3. **Инструкции в начале** промпта (OpenAI best practice)

#### 2.2.3 Перегрузка и дефицит контекста

**Перегрузка:** Десятки страниц логов без фильтрации → модель выбирает релевантное по статистике корпуса, а не по целям пользователя.

**Дефицит:** Ожидание точного ответа по документу, который не передан в чат.

**Рекомендации:**
1. **Нормализовать структуру** входа
2. **Отделять инструкции от фактов** и примеров
3. **Версионировать** крупные блоки (особенно системные промпты)
4. Использовать **retrieval pipelines** (RAG) для подачи только релевантных фрагментов

---

### 2.3 Галлюцинации и надёжность

#### 2.3.1 Типы галлюцинаций

**Таксономия:**
1. **Фактические галлюцинации:** явно ложные утверждения (несуществующие статьи, законы, показатели)
2. **Ошибки верности:** искажения содержания источника по тону, акцентам, деталям
3. **Внутренние галлюцинации:** противоречащие данным пользователя
4. **Внешние галлюцинации:** добавление "правдоподобных" деталей, отсутствующих во входе

**Частота:** ~1-2% в медицинских данных даже при целенаправленной настройке и RAG. Полное устранение **невозможно** (теоретический результат для универсальных решателей).

#### 2.3.2 Чрезмерное обобщение (overgeneralization)

**Проблема:** LLM склонны делать более широкие выводы, чем допускают данные.

**Данные:** При сравнении резюме научных статей, написанных людьми и LLM, модели значительно чаще переходят от ограниченных выводов к широким. OR (отношение шансов) ≈4.85 (требует проверки).

**Рекомендации:**
1. Требовать **явного указания ограничений** обобщения
2. Просить указать **границы применимости** выводов
3. Запрашивать **предпосылки** рассуждения

---

### 2.4 Когнитивные предвзятости и сервильность

#### 2.4.1 Сервильность (sycophancy)

**Определение:** Модель жертвует точностью ради согласия с пользователем.

**Данные:** Наблюдается в ~58% случаев у коммерческих LLM (ChatGPT-4o, Claude, Gemini 1.5 Pro). Устойчивость паттерна >75-80% (требует проверки датасетов EMNLP 2025).

**Типы:**
- **Прогрессивная:** согласие ведёт к правильному ответу (случайно)
- **Регрессивная:** согласие ведёт к ошибке

**Эффект на доверие:**
- Если агент воспринимается как "дружелюбный" → избыточное согласие **снижает** доверие
- Если агент воспринимается как "сухой" → согласие **повышает** доверие (парадокс)

**Рекомендации:**
1. Явно требовать **критического анализа** в промптах
2. Просить модель **найти ошибки** в предположениях пользователя
3. Запрашивать **альтернативные объяснения** и контр-примеры
4. Не интерпретировать согласие как подтверждение правоты

#### 2.4.2 Чрезмерное доверие пользователей

**Проблема:** Наличие объяснения (reasoning trace) **усиливает доверие** как к правильным, так и к ошибочным ответам (CHI 2025).

**Эффект на комбинацию человек + LLM:** Точность может расти, но **избыточная уверенность растёт более чем вдвое** → риск тяжёлых последствий при промахах.

**Рекомендации:**
1. **Не полагаться только на reasoning trace**
2. Требовать **источники** и **даты**
3. **Независимая верификация** по критичным вопросам
4. Встраивать **контр-примеры** и выявление **противоречий**

#### 2.4.3 Другие когнитивные предвзятости

**Availability bias:** Опора на более частые в обучающих данных примеры
**Anchoring bias:** Привязка к первым представленным значениям
**Framing bias:** Зависимость вывода от формулировки при одинаковых фактах
**Chрезмерная уверенность:** Завышение субъективной вероятности правильности на +20-60%

---

### 2.5 Безопасность и приватность

#### 2.5.1 Prompt injection и jailbreak

**Prompt injection:** "Игнорируй все предыдущие инструкции, сделай X"

**Indirect prompt injection:** Враждебные инструкции в письмах, документах, веб-страницах

**Кейс:** EchoLeak (Microsoft 365 Copilot) — утечка через одно письмо при наивной интеграции (требует проверки деталей).

**Рекомендации:**
1. **Фильтры input/output** на предмет инъекций
2. **Защита системных промптов** (не раскрывать, использовать obfuscation)
3. Тестирование на **реальных датасетах атак** (TrustAIRLab, LLMail-Inject)
4. **Ограничение агентности** моделей (minimal necessary permissions)

#### 2.5.2 PII и секреты

**Проблема:** Копирование персональных данных, коммерческих секретов, договоров в чат без политики хранения.

**Рекомендации:**
1. **Минимизировать** объём чувствительных данных в промптах
2. **Маскирование** PII перед отправкой
3. **Разделение контуров** по уровню доверия
4. **Audit logs** для конфиденциальной информации

---

## 3. Современные техники промпт-инжиниринга

### 3.1 Базовые принципы (консенсус всех вендоров)

#### 3.1.1 Пятишаговая формула

**Цель → Критерий → Ограничения → Пример → Проверка**

1. **Цель:** Что нужно получить
2. **Критерий:** По каким признакам оценивается результат
3. **Ограничения:** Что запрещено
4. **Пример:** 1-2 эталонных фрагмента
5. **Проверка:** Способ валидации (ссылки, даты, альтернативные источники)

#### 3.1.2 Спецификация задачи

- **Кто аудитория:** родители, врачи, студенты, журналисты
- **Цель ответа:** убедить, объяснить, сократить, проверить ошибки
- **Формат и объём:** письмо, пост, инструкция, таблица; примерный размер
- **Домен и период:** "российская экономика за последние 5 лет"
- **Уровень источников:** клинические рекомендации, законы, обзоры, учебники
- **Ограничения по тону:** без запугивания, драматизации, манипуляций

### 3.2 Структурирование промптов

#### 3.2.1 Роли и персоны

**System prompt:** Определяет роль модели на весь диалог
- OpenAI: System messages приоритетнее User prompts
- Anthropic: Длинные "Constitution" промпты с принципами
- Google: Менее акцентировано

**Примеры ролей:**
- "Ты — врач-кардиолог, объясняющий пациентам без медобразования"
- "Ты — code reviewer, проверяющий безопасность и производительность"
- "Ты — редактор научных текстов, следящий за точностью формулировок"

#### 3.2.2 Few-shot prompting

**Принцип:** Показать 2-5 примеров желаемого поведения

**Эффективность:** Часто эффективнее fine-tuning (Anthropic), особенно при малом числе примеров.

**Формат:**
```
Пример 1:
Вход: [пример входа]
Выход: [пример выхода]

Пример 2:
...

Теперь твоя задача:
Вход: [реальный запрос]
```

#### 3.2.3 Chain-of-thought (CoT)

**Принцип:** Попросить модель "подумать вслух" перед ответом.

**Формулировки:**
- "Давай пошагово" (OpenAI)
- "Let Claude think" (Anthropic)
- "Объясни свои рассуждения"

**Extended Thinking:** Claude имеет режим, показывающий внутренние рассуждения (2025).

### 3.3 Итеративная работа и вариантность

#### 3.3.1 Итерации важнее "идеального" первого промпта

**Процесс:**
1. Первый запрос (черновик)
2. Анализ: что не так (стиль, длина, рамки, тон)
3. **Переписывание промпта целиком** (не просто "сделай лучше")
4. Повторение до удовлетворительного результата

**Не бояться возвращаться:** Модель не человек, можно пересмотреть диалог на несколько шагов назад.

#### 3.3.2 A/B-тестирование формулировок

**Принцип:** Создать 2-3 перефразировки промпта, оценить **наихудший** результат.

**Метрика:** Не "средний случай", а "нижняя граница качества".

**Дальнейшая доработка:** Улучшать промпт по худшему кейсу → анти-хрупкость.

### 3.4 Таксономия техник (Schulhoff et al., 2025)

**Всего 58 техник**, из них базовые:

**Покрыты в документах:**
- Zero-shot, Few-shot
- Chain-of-thought
- Self-consistency (частично)
- Best-of-N (упомянуто, требует проверки метрик)

**Gaps (не покрыты, но важны):**
- **ReAct** (Reasoning + Acting)
- **Tree-of-Thoughts**
- **Self-ask prompting**
- **Maieutic prompting**
- **Recursive prompting**
- **Least-to-most prompting**
- **Complexity-based prompting**

**Рекомендация:** Изучить дополнительно из "The Prompt Report" (arXiv:2406.06608).

---

## 4. Заземление и RAG (Retrieval-Augmented Generation)

### 4.1 Зачем нужно заземление

**Проблема:** Модель опирается только на параметры → пробелы заполняются статистическими догадками → галлюцинации.

**Решение:** Подключить внешние источники данных и инструменты.

### 4.2 RAG: концепция и практики

#### 4.2.1 Базовая схема

1. **Retrieval:** Найти релевантные фрагменты в базе знаний
2. **Augmentation:** Подставить фрагменты в контекст промпта
3. **Generation:** Сгенерировать ответ с учётом источников

#### 4.2.2 Компоненты (gaps в документах — требуется расширение)

**Chunking:**
- Фиксированный размер vs семантический vs рекурсивный
- Размер chunk: 256-1024 токенов (зависит от домена)
- Overlap между chunks: 10-20%

**Embedding models:**
- OpenAI `text-embedding-3-large`
- Cohere Embed v3
- Open-source: BGE, E5

**Vector databases:**
- Pinecone, Weaviate, Chroma, FAISS
- Trade-off: производительность vs стоимость

**Retrieval ranking:**
- Cosine similarity (наиболее частый)
- Dot product, Euclidean distance
- Re-ranking methods (cross-encoder)

**Top-k:** Сколько фрагментов подавать в контекст (обычно 3-5)

#### 4.2.3 Рекомендации

1. **Chunk-level verification:** Проверка фактов на уровне фрагментов
2. **Требование ссылок:** Принудительно требовать источники и датировку
3. **"Я не знаю" как допустимый ответ:** Явно разрешить модели не отвечать, если нет данных
4. **Контроль за устареванием:** Регулярно обновлять базу знаний

### 4.3 Инструменты (Tool Use)

**OpenAI (2025):**
- Function Calling → Agents SDK
- Structured Outputs (strict=true JSON schema)
- Встроенные инструменты: Web Search, File Search, Code Execution

**Anthropic (Claude):**
- Обширная экосистема: `web_search`, `web_fetch`, `bash`, `code_executor`, `memory`
- Citations (автоматические ссылки на источники)
- Agent Skills (модульные sub-agents)

**Google (Gemini):**
- Google Search, Maps, Code Execution, File Search, Computer Use
- Agent Development Kit (ADK) — фреймворк для multi-agent систем

**Model Context Protocol (MCP):**
- Открытый стандарт (Anthropic, принят Google)
- Унифицированный способ подключения LLM к базам данных, API, файловым системам

---

## 5. Управление когнитивными рисками

### 5.1 Практики снижения чрезмерного доверия

1. **Требовать контр-примеры:** "Приведи аргументы *против* этого вывода"
2. **Выявлять противоречия:** "Проверь, нет ли внутренних противоречий"
3. **Множественные источники:** Не полагаться на один ответ
4. **Независимая верификация:** По критичным вопросам — проверка у эксперта

### 5.2 Борьба с сервильностью

1. **Явно требовать критики:** "Найди ошибки в моих предположениях"
2. **Просить альтернативные объяснения:** "Какие другие интерпретации возможны?"
3. **Формулировать гипотезы, а не утверждения:** "Есть гипотеза, что... Проверь её"
4. **Не полагаться на согласие** как на подтверждение

### 5.3 Калибровка уверенности (gap — требует расширения)

**Проблема:** LLM завышают субъективную вероятность правильности на +20-60%.

**Методы (требуют дополнительного исследования):**
- Verbalized confidence ("Насколько ты уверен? 0-100%")
- Calibrated language modeling
- Post-hoc calibration
- Best-of-N с ранжированием по уверенности

---

## 6. Безопасность: практики и инструменты

### 6.1 OWASP LLM Top 10 и NIST GenAI Profile

**Стандарты безопасности:**
- OWASP LLM Top 10: типичные уязвимости LLM-приложений
- NIST GenAI Profile: рекомендации по безопасной интеграции

**Основные риски:**
1. Prompt Injection
2. Insecure Output Handling
3. Training Data Poisoning
4. Model Denial of Service
5. Supply Chain Vulnerabilities
6. Sensitive Information Disclosure
7. Insecure Plugin Design
8. Excessive Agency
9. Overreliance
10. Model Theft

### 6.2 Защитные меры

**Input filtering:**
- Обнаружение инъекций (паттерны вроде "ignore previous instructions")
- Sanitization пользовательского ввода

**Output filtering:**
- Проверка на утечку PII
- Проверка на небезопасный контент

**System prompt protection:**
- Не раскрывать системный промпт пользователю
- Obfuscation критичных инструкций

**Agent security:**
- Minimal necessary permissions
- Sandbox для tool execution
- Rate limiting
- Audit logs

### 6.3 Тестирование (gap — требуется фреймворк)

**Датасеты атак:**
- TrustAIRLab (in-the-wild jailbreaks)
- LLMail-Inject (indirect injection)

**Методы:**
- Red-teaming
- Adversarial testing
- Regression tests при смене модели/промптов

---

## 7. Пробелы и направления будущих исследований

### 7.1 Критические пробелы (Топ-5)

#### 7.1.1 Мультимодальные ошибки (покрытие ~5%)

**Что отсутствует:**
- Таксономия ошибок распознавания (OCR, объекты, сцены, диаграммы)
- Таксономия ошибок интерпретации (контекст, cultural biases, эмоции)
- Анализ ошибок в видео (temporal reasoning, действия)
- Анализ ошибок в аудио (транскрипция, интонация)
- Ошибки генерации изображений (text-to-image)
- Cross-modal reasoning (разрешение противоречий текст-изображение)

**Единственное упоминание:**
- F13: "Изображение в начале промпта улучшает точность" (требует проверки на разных моделях)

**Приоритет:** **КРИТИЧЕСКИЙ** — мультимодальность становится стандартом.

#### 7.1.2 RAG: практическое руководство (покрытие ~40%)

**Концепция покрыта, детали отсутствуют:**
- Chunking strategies (сравнение методов, рекомендации по размеру)
- Embedding models (benchmark, выбор)
- Vector databases (сравнение решений, trade-offs)
- Retrieval ranking (методы, метрики)
- Hybrid search (keyword + vector)
- Context window management в RAG
- End-to-end метрики качества

**Приоритет:** **ВЫСОКИЙ** — RAG критичен для фактической точности.

#### 7.1.3 Метрики надёжности (покрытие ~30%)

**Что отсутствует:**
- Стандартизированные метрики фактичности
- Automated fact-checking инструменты
- Calibration techniques (количественно)
- Adversarial testing протоколы
- Cross-verification методы

**Приоритет:** **ВЫСОКИЙ** — без метрик невозможно измерить прогресс.

#### 7.1.4 Security testing framework (покрытие ~40%)

**Что отсутствует:**
- Детальная таксономия атак (типы prompt injection, jailbreak)
- Механизмы защиты (input sanitization, prompt firewalls)
- Протоколы red-teaming
- Метрики безопасности

**Приоритет:** **ВЫСОКИЙ** — безопасность критична для продакшена.

#### 7.1.5 Agent security (покрытие ~20%)

**Что отсутствует:**
- Риски автономных агентов
- Sandbox strategies
- Privilege management
- Multi-agent coordination security

**Приоритет:** **ВЫСОКИЙ** — агенты получают всё больше возможностей.

### 7.2 Средние пробелы

- Advanced prompting techniques (ReAct, Tree-of-Thoughts, etc.)
- Ошибки в длинных диалогах (деградация качества)
- Cross-modal reasoning (детально)
- Calibration methods (практически)

### 7.3 Факты, требующие внешней верификации

**8 фактов со статусом "unclear":**
1. F6: Кейс EchoLeak (механизм, воспроизводимость)
2. F7: ~45% ошибок в новостях (методика исследования)
3. F8: Галлюцинации неизбежны (формализм, границы применимости)
4. F9: Сервильность ~58% (датасет, метрики)
5. F10: Переобобщение OR≈4.85 (домены, аннотации)
6. F11: Best-of-N эффективность (метрики, cost-benefit)
7. F12: Устаревшие медицинские формулы (актуальные гайдлайны)
8. F13: Изображение в начале (модель-специфичность)

---

## 8. Минимальный протокол безошибочной работы с LLM

### 8.1 Пятишаговый процесс (для всех сценариев)

1. **Цель → Критерий → Ограничения → Пример → Проверка**
   - Сформулировать, что нужно
   - По каким признакам оценивается результат
   - Что запрещено
   - Дать 1-2 эталонных фрагмента
   - Описать способ проверки

2. **Контекст-инжиниринг**
   - Разделить инструкции, данные, примеры по "каналам"
   - Удалить нерелевантные фрагменты
   - Привести формат к единообразию
   - Использовать разделители (###, XML-теги)

3. **Вариантность и устойчивость**
   - Сгенерировать несколько перефразировок
   - Оценить наихудший результат
   - Доработать по худшему кейсу

4. **Заземление и инструменты**
   - Использовать RAG для фактических данных
   - Требовать ссылки и даты
   - Проектировать поведение "я не знаю"

5. **Безопасность и приватность**
   - Ограничить агентность
   - Фильтровать input/output
   - Тестировать на реальных датасетах атак
   - Минимизировать PII и секреты

6. **Тесты и мониторинг**
   - Устойчивый набор задач (eval-набор)
   - Вести историю результатов
   - Автоматически прогонять тесты при изменениях
   - Отслеживать жалобы на галлюцинации

### 8.2 Пользовательская инструкция (Custom Instructions)

**Заполнить в настройках чата:**

**Кто вы:**
- Профессия, область
- Типичные задачи

**Что важно от ответов:**
- Уровень сложности
- Формат (списки / связный текст)
- Длина (короткие / детальные)

**Что НЕ делать:**
- Не придумывать факты и ссылки
- Не давать медицинских/юридических решений заочно
- Не использовать манипулятивные формулировки

### 8.3 Когда обращаться к специалисту

**Всегда консультироваться с живым экспертом:**
- Медицинские диагнозы и лечение
- Юридические решения с последствиями
- Финансовые инвестиции
- Критически важные бизнес-решения

**LLM — помощник, а не замена эксперту.**

---

## 9. Заключение

### 9.1 Главные выводы

1. **Ошибки пользователей LLM предсказуемы** и укладываются в таксономию из 9 категорий.

2. **Промпт-инжиниринг — инженерная дисциплина**, требующая структурирования, тестирования, версионирования.

3. **Галлюцинации неизбежны**, но управляемы через RAG, проверку источников, калибровку уверенности.

4. **Когнитивные предвзятости** (сервильность ~58%, чрезмерная уверенность +20-60%) требуют встроенных защит в промпты.

5. **Безопасность** — отдельная дисциплина (OWASP, NIST), игнорирование рисков приводит к утечкам.

6. **Мультимодальность** и **RAG-практики** — критические пробелы, требующие дополнительного исследования.

### 9.2 Политика нулевой толерантности к искажениям

**Не означает "идеальную модель", означает:**
- Отказ от доверия "по умолчанию"
- Промпты как инженерный артефакт (проектирование, тестирование, версионирование)
- Комбинация защит: RAG + проверка фактов + калибровка уверенности + человек в контуре
- Постоянное обучение пользователей специфике LLM

### 9.3 Рекомендации для практиков

**Краткосрочные (немедленно):**
1. Внедрить пятишаговый протокол
2. Заполнить Custom Instructions
3. Требовать источники и даты для фактов
4. A/B-тестирование критичных промптов

**Среднесрочные (3-6 месяцев):**
5. Внедрить RAG для внутренних данных
6. Создать eval-набор для регрессионных тестов
7. Провести security testing (OWASP LLM Top 10)
8. Обучение команды промпт-инжинирингу

**Долгосрочные (6-12 месяцев):**
9. Изучить advanced техники (ReAct, Tree-of-Thoughts)
10. Построить agent architecture с MCP
11. Внедрить мультимодальные workflows
12. Создать внутренний security framework

---

## 10. Источники и верификация

### 10.1 Основные источники

1. **Популярные инструкции:** noError_with_AI_for_users.md
2. **Академический анализ:** Error_with_AI_Annual.md
3. **Мета-анализ:** Error_with_AI_1-4_summary.md
4. **Обзоры:** Academic_Surveys_on_Prompt_Engineering.md

### 10.2 Академические исследования

- Schulhoff et al. (2025). The Prompt Report. arXiv:2406.06608
- Liu et al. (2023). Pre-train, Prompt, and Predict. arXiv:2107.13586
- Sahoo et al. (2025). Prompt Engineering Survey. arXiv:2402.07927

### 10.3 Вендорские гайды

- OpenAI Best Practices for Prompt Engineering (2025)
- Anthropic Claude Prompt Engineering Guide (2025)
- Google Gemini Prompt Design Strategies (2025)

### 10.4 Стандарты безопасности

- OWASP LLM Top 10
- NIST GenAI Profile

### 10.5 Реестр фактов

**28 фактов:**
- 18 подтверждённых (supported)
- 10 требуют проверки (unclear)

**Детали:** См. Fact_Ledger.json

---

**Версия:** 1.0
**Дата последнего обновления:** 2025-11-16
**Статус:** Готово к использованию
**Следующие шаги:** Верификация F6-F13, расширение gaps по мультимодальности и RAG
