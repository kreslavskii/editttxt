## Academic Surveys on Prompt Engineering (2022–2025)

**Schulhoff et al. (2025) – “The Prompt Report”:** A comprehensive, peer-reviewed survey assembling a taxonomy of *58* distinct prompting techniques for LLMs[\[1\]](https://arxiv.org/abs/2406.06608#:~:text=taxonomy%20of%2058%20LLM%20prompting,entire%20literature%20on%20natural%20language). This report standardizes terminology (33 defined terms) and catalogs prompting methods across tasks and modalities. It also compiles best practices and guidelines for effective prompts – including specific advice for state-of-the-art models like ChatGPT[\[1\]](https://arxiv.org/abs/2406.06608#:~:text=taxonomy%20of%2058%20LLM%20prompting,entire%20literature%20on%20natural%20language). (Confidence: High, as it’s the most up-to-date systematic survey).

**Liu et al. (2023) – “Pre-train, Prompt, and Predict”:** A foundational survey introducing the *prompt-based learning* paradigm[\[2\]](https://arxiv.org/abs/2107.13586#:~:text=,This%20framework%20is). It presents a unified mathematical framework and a structured typology of prompting methods in NLP[\[3\]](https://arxiv.org/abs/2107.13586#:~:text=mathematical%20notations%20that%20can%20cover,updated%20survey%2C%20and%20paperlist), covering early prompt engineering approaches (manual and automated prompts, few-shot learning) that informed later techniques. (Confidence: High, well-cited classical survey).

**Sahoo et al. (2025) – Prompt Engineering Techniques & Applications:** A shorter systematic survey that categorizes prompt engineering advances by application domain (QA, reasoning, etc.)[\[4\]](https://arxiv.org/abs/2402.07927#:~:text=there%20remains%20a%20lack%20of,better%20understanding%20of%20this%20rapidly). For each prompting approach, it summarizes the methodology, typical use cases, model examples, and datasets, and analyzes strengths/limitations[\[5\]](https://arxiv.org/abs/2402.07927#:~:text=the%20gap%20by%20providing%20a,and%20opportunities%20for%20prompt%20engineering). It includes a taxonomy diagram and highlights open challenges and research opportunities in prompt engineering[\[5\]](https://arxiv.org/abs/2402.07927#:~:text=the%20gap%20by%20providing%20a,and%20opportunities%20for%20prompt%20engineering). (Confidence: High, focuses on practical categorization by use-case).

*Other notable references:* Ongoing literature curation projects (e.g. **PromptingGuide.ai**) track new prompting papers and techniques. Additionally, specialized reviews (e.g. on prompt attacks or domain-specific prompting) have emerged, but the above surveys form the core **reference corpus** for general prompt engineering up to 2025.

## OpenAI (ChatGPT/GPT-4) – Official Prompting & Tool Use Guides

**OpenAI Prompting Best Practices (2023–2025):** OpenAI’s documentation and help guides provide clear rules of thumb for prompt design. Key recommendations include placing instructions at the *beginning* of the prompt and delineating them from context (e.g. using `###` or triple quotes)[\[6\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=2,separate%20the%20instruction%20and%20context), and being as specific and explicit as possible about the desired output format, style, and content[\[7\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=3,outcome%2C%20length%2C%20format%2C%20style%2C%20etc). For example, rather than a vague request (“Write a poem about X”), the guide suggests providing detailed context and style cues (“Write a **short, inspiring** poem about X in the style of Y…”)[\[8\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=Less%20effective%20%E2%9D%8C%3A). Such practices, along with iterative refinement (zero-shot, then few-shot, then fine-tuning if needed), are officially encouraged to improve ChatGPT’s responses[\[9\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=Text%3A%20)[\[10\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=%E2%9C%85%20Few,couple%20of%20examples). (Confidence: High, sourced from OpenAI’s published guidelines).

**OpenAI Function Calling and “Agents” (2024–2025):** In mid-2023, OpenAI introduced *function calling* for GPT-3.5/4, allowing the model to invoke external functions (tools) by returning a JSON function call. By early 2025 this evolved into the first version of an OpenAI “Agents” platform[\[11\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=As%20of%20March%2011%2C%202025%2C,our%20Agents%20SDK%20with%20Tracing). OpenAI’s API documentation (updated March 2025) describes new built-in tools (e.g. web search, file retrieval, code execution) and an **Agents SDK** for building agentic applications[\[11\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=As%20of%20March%2011%2C%202025%2C,our%20Agents%20SDK%20with%20Tracing). The official guide explains that function calling lets developers *“connect OpenAI models to external tools and systems”*, enabling AI assistants to fetch real-time data, take actions (e.g. schedule a meeting), or perform computations on behalf of the user[\[12\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=Function%20calling%20allows%20you%20to,between%20your%20applications%20and%20LLMs)[\[13\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=Function%20calling%20is%20useful%20for,of%20use%20cases%2C%20such%20as). OpenAI also added features like *Structured Outputs* (with a `strict=true` JSON schema enforcement for function call arguments) to increase reliability[\[14\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=In%20June%202024%2C%20we%20launched,provided%20in%20the%20function%20definition). (Confidence: High, based on OpenAI’s help center and release notes).

**System & Role Instructions:** Although not a single page, OpenAI’s documentation emphasizes the usage of system messages (or “developer” messages) to steer the assistant’s behavior. The **OpenAI model specification** (as of late 2025) clarifies that the model prioritizes system instructions over user prompts. Official examples suggest using the system role to define style/tone or domain-specific behavior (e.g. *“You are a helpful legal assistant…”*). While OpenAI hasn’t released a detailed public guide solely on system prompting, their help articles and community posts underscore that clear role instructions in the system message can significantly shape ChatGPT’s responses[\[15\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=Less%20effective%20%E2%9D%8C%3A)[\[16\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=The%20following%20is%20a%20conversation,com%2Fhelp%2Ffaq). (Confidence: Medium, drawn from various OpenAI communications; the behavior is confirmed by OpenAI but detailed guidelines are somewhat scattered).

**Retrieval-Augmented Generation (RAG) with OpenAI:** OpenAI does not provide a proprietary RAG tool out-of-the-box, but they officially support practices like embedding-based retrieval. The OpenAI Cookbook (an official example repository) includes tutorials on using vector databases with GPT-4. While not formal documentation, OpenAI’s *“Best practices”* note that providing relevant context documents in the prompt (such as a retrieved knowledge snippet) can help reduce hallucinations. In 2025, OpenAI’s introduction of the function-based **File Search** tool[\[11\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=As%20of%20March%2011%2C%202025%2C,our%20Agents%20SDK%20with%20Tracing) essentially built RAG into the Agents API (allowing the model to query an index). (Confidence: Medium, as details are from indirect sources: examples and the new tool feature).

## Anthropic (Claude) – Official Documentation & Guides

**Claude Prompt Engineering Guide (Anthropic Docs, 2025):** Anthropic provides an extensive official guide to prompt engineering for Claude models[\[17\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=)[\[18\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,Extended%20thinking%20tips). It covers general best practices similar to OpenAI’s (e.g. *“Be clear and direct”*[\[19\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,68), avoid ambiguity) and Claude-specific tips. Notably, the guide includes techniques like **few-shot examples** (“Use examples (multishot prompting)” in the docs), **chain-of-thought prompting** (“Let Claude think”) to encourage step-by-step reasoning[\[20\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,70), and use of structured formatting (Anthropic suggests XML tags or markdown headings to organize complex prompts[\[21\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=We%20recommend%20organizing%20prompts%20into,as%20models%20become%20more%20capable)). It also emphasizes defining a clear *role or persona* for Claude via the system prompt (“Give Claude a role”)[\[18\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,Extended%20thinking%20tips), which aligns with Anthropic’s focus on harmless and helpful AI. The documentation is very hands-on, even offering a *Prompt Generator* tool in the Claude Console to help draft prompts[\[22\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=1,prompt%20you%20want%20to%20improve). (Confidence: High, directly from Anthropic’s official docs).

**Claude’s Tool Use and Agentic Features:** Anthropic has integrated an **extensive tool ecosystem** natively into Claude’s API. Official documentation lists tools such as web search, web page fetching, a sandboxed code executor, a Bash shell, and even a *“Memory”* for long-term context[\[23\]](https://docs.claude.com/en/api/overview#:~:text=These%20features%20enable%20Claude%20to,line%20operations.Claude%20API)[\[24\]](https://docs.claude.com/en/api/overview#:~:text=MCP%20connector%20Connect%20to%20remote,Claude%20API%20%28Beta). These allow Claude to perform actions or retrieve information during a conversation. For example, Claude can call a `web_search` tool to augment its knowledge with current data[\[25\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20Web,Claude%20API), or use the Bash tool to execute commands. Anthropic’s **“Prompt tools”** interface (in experimental API features) lets developers implement custom tools and define how Claude should invoke them[\[26\]](https://docs.claude.com/en/api/overview#:~:text=Tool%20useEnable%20Claude%20to%20interact,Claude%20API). Moreover, Anthropic’s platform supports *Citations* – when given source documents, Claude can attach reference markers in its answers[\[27\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20CitationsGround,Claude%20API) – and *Search Results* formatting for retrieved info[\[28\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20Search,Claude%20API)[\[29\]](https://docs.claude.com/en/api/overview#:~:text=search%20results%20with%20proper%20source,Claude%20API). These features show Claude’s slant toward retrieval-augmented generation and transparent reasoning. (Confidence: High, based on Claude’s official feature list[\[27\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20CitationsGround,Claude%20API)[\[25\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20Web,Claude%20API)).

**Anthropic’s Agent “Skills” and MCP:** In 2025, Anthropic introduced **Agent Skills** – essentially modular sub-agents or functions that Claude can invoke for complex tasks (e.g. a “Slack assistant” skill). Their documentation encourages designing *skills* with clear instructions and even scripting, which can be seen as higher-level prompt engineering (wrapping prompts into re-usable modules). Anthropic also co-developed the **Model Context Protocol (MCP)**, an open standard for connecting AI assistants to external data and services[\[30\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Today%2C%20we%27re%20open,produce%20better%2C%20more%20relevant%20responses)[\[31\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Model%20Context%20Protocol). In a November 2024 announcement, Anthropic open-sourced MCP to enable secure two-way integration between Claude and company databases, knowledge bases, etc., without custom APIs[\[32\]](https://www.anthropic.com/news/model-context-protocol#:~:text=assistants%20to%20the%20systems%20where,produce%20better%2C%20more%20relevant%20responses)[\[31\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Model%20Context%20Protocol). Claude 3.5 (and later) supports MCP natively – for instance, Claude’s desktop app can connect to an MCP server that interfaces with Google Drive or a SQL database[\[33\]](https://www.anthropic.com/news/model-context-protocol#:~:text=,source%20repository%20of%20MCP%20servers)[\[34\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Claude%203,GitHub%2C%20Git%2C%20Postgres%2C%20and%20Puppeteer). This allows Claude to fetch internal documents or execute actions as a true “agent” while keeping the context window updated. Anthropic’s engineering blog further coins the term **“context engineering”** for managing what information to include over multi-turn dialogues, highlighting techniques to maintain optimal context length and content as an evolution beyond single-prompt engineering[\[35\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=After%20a%20few%20years%20of,generate%20our%20model%E2%80%99s%20desired%20behavior)[\[36\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=Context%20engineering%20vs). (Confidence: High, info from Anthropic’s official news and engineering posts).

**Claude vs OpenAI nuances:** According to both official guides and practical studies, Claude tends to follow lengthy, structured system prompts very faithfully (Anthropic often demos a long Constitution of principles as a system prompt for Claude). The Anthropic documentation acknowledges that “prompt engineering…particularly **system prompts**…is key to steer behavior”[\[37\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=In%20the%20early%20days%20of,tools%2C%20%206%20Model%20Context), but also notes that as agents get more autonomous (taking multiple actions via tools), *managing the evolving context* (what to keep or discard from conversation history, tool outputs, etc.) becomes crucial[\[38\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=generation%20tasks,external%20data%2C%20message%20history%2C%20etc)[\[39\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=inference%20and%20longer%20time%20horizons%2C,external%20data%2C%20message%20history%2C%20etc). This reflects in Claude’s features like **Context Editing** (automatic removal of irrelevant context when near token limits) and the *Extended Thinking* mode that reveals intermediate reasoning steps[\[40\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20,Claude%20API). Those aspects are less emphasized in OpenAI’s user-facing docs. (Confidence: Medium, based on interpretation of official features; some comparative insight from secondary practical evaluations).

## Google (Gemini/Bard) – Official Guides and Frameworks

**Google’s Prompt Design Guides (Gemini, 2024–2025):** Google (and DeepMind) released extensive prompting documentation alongside the Gemini model launch. The *Google AI for Developers* site provides a “**Prompt Design Strategies**” guide for Gemini models[\[41\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=that%20elicit%20accurate%2C%20high%20quality,responses%20from%20a%20language%20model), which covers best practices analogous to other vendors. It emphasizes clear, specific instructions and offers example prompts with *input/output pairs*. For instance, Google’s guide suggests mapping out the user’s intent in detail and even providing step-by-step input formats (for multi-step tasks) to elicit high-quality responses[\[41\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=that%20elicit%20accurate%2C%20high%20quality,responses%20from%20a%20language%20model). The documentation notes that Gemini often performs well *“without the need for prompt engineering”* on straightforward asks, but complex tasks still *“benefit from effective prompt engineering”*[\[42\]](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#:~:text=Gemini%20models%20often%20perform%20well,still%20plays%20an%20important%20role) – a subtle hint that Gemini’s instruction-following might be highly tuned (likely due to extensive instruction fine-tuning). Google also published domain-specific prompt tips (e.g. prompting for code vs. for image generation with Imagen). An official **Gemini Prompting 101** whitepaper (68 pages, late 2024) circulated in developer communities, indicating Google’s internal research on prompt patterns (though we rely here on the summarized guides). (Confidence: High, drawn from Google’s official dev documentation[\[41\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=that%20elicit%20accurate%2C%20high%20quality,responses%20from%20a%20language%20model)[\[42\]](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#:~:text=Gemini%20models%20often%20perform%20well,still%20plays%20an%20important%20role)).

**Gemini API – Tools and Function Calling:** Google’s Gemini API (part of Vertex AI and Google Cloud) supports function calling and a suite of **tools** very similar to OpenAI and Anthropic’s offerings. The official docs list built-in tools like *Google Search*, *Maps*, *Code Execution*, *URL context fetching*, *File Search*, and even a *“Computer Use”* tool for GUI actions[\[43\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=)[\[44\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=,Live%20API). These allow a Gemini-powered assistant to perform web queries, run code, look up files, or control a simulated computer environment. Google’s approach to tool use is standardized under the **Model Context Protocol (MCP)** as well – notably, Google adopted MCP (originating from Anthropic) to let Gemini interface with external data in a secure, consistent way[\[45\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=The%20Model%20Context%20Protocol%20,more%20accurate%2C%20useful%2C%20and%20automated)[\[46\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=The%20MCP%20creates%20a%20standardized%2C,included%20in%20their%20original%20training). Official Google Cloud guides (2024) describe MCP as a *“bridge allowing LLMs to become dynamic agents that retrieve real-time information and take actions”*[\[47\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=Large%20language%20models%20,or%20updating%20a%20customer%20record). In practice, a Gemini agent can request an MCP client to, say, query a database or invoke a transactional API, and then incorporate the results back into its response[\[48\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=At%20its%20core%2C%20the%20Model,email%20it%20to%20my%20manager)[\[49\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=1,the%20company%27s%20database%2C%20and%20retrieves). This is analogous to OpenAI’s function calling, but MCP is an open standard supported by multiple vendors (Anthropic Claude, Google Gemini, etc.) and aims for interoperability[\[46\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=The%20MCP%20creates%20a%20standardized%2C,included%20in%20their%20original%20training)[\[50\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=existing%20concepts%20like%20tool%20use,included%20in%20their%20original%20training). (Confidence: High, per Google’s documentation and blog).

**Agent Development Kit (ADK) – Multi-Agent Systems:** In April 2025, Google introduced the **Agent Development Kit (ADK)** as an open-source framework for building complex agent systems[\[51\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=intelligent%2C%20autonomous%20multi,greater%20flexibility%20and%20precise%20control)[\[52\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=Agent%20Development%20Kit%20cloud,greater%20flexibility%20and%20precise%20control). ADK is essentially Google’s answer to orchestrating LLM-powered “agents” that can plan, use tools, and work together. The Google Developers Blog announcement highlights ADK’s support for **hierarchical multi-agents** (planner/executor patterns), integration of various models (Gemini or third-party via Vertex AI), and a rich tool ecosystem including MCP tools and even other agents as tools[\[53\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=integration%20letting%20you%20choose%20from,AI21%20Labs%2C%20and%20many%20more). ADK provides structures for defining an agent’s *role/instructions*, its available tools (with tool schemas), and memory, along with a workflow engine for sequential or parallel task execution[\[54\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=%2A%20Multi,Enable%20complex%20coordination%20and%20delegation)[\[55\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=,transfer%29%20for%20adaptive%20behavior). In effect, it formalizes prompt engineering at the system level: developers explicitly write *agent prompts* (system messages describing each agent’s role and objective), define how agents communicate, and manage the shared context. This is aligned with ideas from research on LLM *planning and self-play*, but packaged for enterprise use. Google’s ADK documentation and codelabs give examples like a “travel assistant” composed of multiple sub-agents, demonstrating how prompts and code come together to yield more autonomous behaviors. (Confidence: High, based on Google’s official blog and docs[\[56\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=intelligent%2C%20autonomous%20multi,greater%20flexibility%20and%20precise%20control)[\[53\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=integration%20letting%20you%20choose%20from,AI21%20Labs%2C%20and%20many%20more)).

**Comparative Note – Gemini vs GPT-4/Claude:** While official data is limited, Google has hinted that **Gemini** (especially larger versions like “Gemini 2.5”) is heavily trained on dialog and code, potentially reducing the need for elaborate prompts on simple tasks[\[42\]](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#:~:text=Gemini%20models%20often%20perform%20well,still%20plays%20an%20important%20role). However, Google’s own best-practice guides still encourage prompt techniques like providing step-by-step examples and specifying output schemas for complex tasks – indicating that, despite any architectural advantages, thoughtful prompt construction remains important. One unique aspect in Google’s ecosystem is the tight integration with Google Workspace and search: e.g. a *“Prompting with Google Workspace”* guide shows how enterprise users can write prompts to query Gmail or Docs via Gemini. This suggests that prompt engineering for Gemini may extend to configuring it as an **enterprise assistant** (with context like corporate data), which differs from the more general-purpose orientation of OpenAI and Anthropic guides. (Confidence: Medium, based on interpretation of Google’s product positioning and scattered docs).

**Summary of Main Corpus:** The sources gathered above will serve as the primary references for our subsequent analysis. The academic surveys (Prompt Report, etc.) provide a taxonomy and terminology for prompt techniques. The official vendor documents (OpenAI, Anthropic, Google) detail the **supported prompting features and guidelines** in each ecosystem – from basic prompt writing tips to advanced tool-use and agent frameworks (e.g. OpenAI’s function calls, Anthropic’s skills, Google’s ADK). This corpus ensures we ground the research in authoritative information and can compare **prompting techniques vs. platform capabilities** across ChatGPT, Claude, and Gemini, as of late 2025. All claims moving forward will be linked back to these sources to uphold the zero-tolerance for unsupported assertions. (Confidence: High, we have covered the key references as requested, and they will be used to build the analysis in the next steps.)

**Sources (for Step 2):**

- **Academic Surveys:** Schulhoff et al. (2025), *“The Prompt Report”*[\[1\]](https://arxiv.org/abs/2406.06608#:~:text=taxonomy%20of%2058%20LLM%20prompting,entire%20literature%20on%20natural%20language); Liu et al. (2023, ACM CSUR), *Prompting Methods Survey*[\[2\]](https://arxiv.org/abs/2107.13586#:~:text=,This%20framework%20is)[\[3\]](https://arxiv.org/abs/2107.13586#:~:text=mathematical%20notations%20that%20can%20cover,updated%20survey%2C%20and%20paperlist); Sahoo et al. (2025), *LLM Prompt Engineering Survey*[\[4\]](https://arxiv.org/abs/2402.07927#:~:text=there%20remains%20a%20lack%20of,better%20understanding%20of%20this%20rapidly).
- **OpenAI Documentation:** OpenAI Help Center – *Best Practices for Prompt Engineering*[\[6\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=2,separate%20the%20instruction%20and%20context)[\[7\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=3,outcome%2C%20length%2C%20format%2C%20style%2C%20etc); OpenAI Help Center – *Function Calling & Agents*[\[11\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=As%20of%20March%2011%2C%202025%2C,our%20Agents%20SDK%20with%20Tracing)[\[12\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=Function%20calling%20allows%20you%20to,between%20your%20applications%20and%20LLMs); OpenAI API Reference (2025) – function calling/JSON mode[\[14\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=In%20June%202024%2C%20we%20launched,provided%20in%20the%20function%20definition).
- **Anthropic Documentation:** Claude Developer Docs – *Prompt Engineering Overview*[\[18\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,Extended%20thinking%20tips) and sub-pages (2025); Anthropic Claude API Features – *Tools and Retrieval (Citations, etc.)*[\[27\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20CitationsGround,Claude%20API)[\[25\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20Web,Claude%20API); Anthropic Announcement – *Model Context Protocol (Nov 2024)*[\[30\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Today%2C%20we%27re%20open,produce%20better%2C%20more%20relevant%20responses)[\[31\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Model%20Context%20Protocol); Anthropic Engineering Blog – *“Context Engineering”* (Sept 2025)[\[35\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=After%20a%20few%20years%20of,generate%20our%20model%E2%80%99s%20desired%20behavior)[\[36\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=Context%20engineering%20vs).
- **Google/DeepMind Resources:** Google AI Developers Prompt Guide – *Gemini Prompt Strategies*[\[41\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=that%20elicit%20accurate%2C%20high%20quality,responses%20from%20a%20language%20model)[\[42\]](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#:~:text=Gemini%20models%20often%20perform%20well,still%20plays%20an%20important%20role); Google Cloud Vertex AI Docs – *Introduction to Prompt Design*【41†L1-L4}; Google Developers Blog – *Agent Development Kit (Apr 2025)*[\[56\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=intelligent%2C%20autonomous%20multi,greater%20flexibility%20and%20precise%20control)[\[53\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=integration%20letting%20you%20choose%20from,AI21%20Labs%2C%20and%20many%20more); Google Cloud Blog – *What is MCP?*[\[45\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=The%20Model%20Context%20Protocol%20,more%20accurate%2C%20useful%2C%20and%20automated)[\[46\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=The%20MCP%20creates%20a%20standardized%2C,included%20in%20their%20original%20training).

------------------------------------------------------------------------

[\[1\]](https://arxiv.org/abs/2406.06608#:~:text=taxonomy%20of%2058%20LLM%20prompting,entire%20literature%20on%20natural%20language) \[2406.06608\] The Prompt Report: A Systematic Survey of Prompt Engineering Techniques

<https://arxiv.org/abs/2406.06608>

[\[2\]](https://arxiv.org/abs/2107.13586#:~:text=,This%20framework%20is) [\[3\]](https://arxiv.org/abs/2107.13586#:~:text=mathematical%20notations%20that%20can%20cover,updated%20survey%2C%20and%20paperlist) \[2107.13586\] Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

<https://arxiv.org/abs/2107.13586>

[\[4\]](https://arxiv.org/abs/2402.07927#:~:text=there%20remains%20a%20lack%20of,better%20understanding%20of%20this%20rapidly) [\[5\]](https://arxiv.org/abs/2402.07927#:~:text=the%20gap%20by%20providing%20a,and%20opportunities%20for%20prompt%20engineering) \[2402.07927\] A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications

<https://arxiv.org/abs/2402.07927>

[\[6\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=2,separate%20the%20instruction%20and%20context) [\[7\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=3,outcome%2C%20length%2C%20format%2C%20style%2C%20etc) [\[8\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=Less%20effective%20%E2%9D%8C%3A) [\[9\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=Text%3A%20) [\[10\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=%E2%9C%85%20Few,couple%20of%20examples) [\[15\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=Less%20effective%20%E2%9D%8C%3A) [\[16\]](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api#:~:text=The%20following%20is%20a%20conversation,com%2Fhelp%2Ffaq) Best practices for prompt engineering with the OpenAI API \| OpenAI Help Center

<https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-the-openai-api>

[\[11\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=As%20of%20March%2011%2C%202025%2C,our%20Agents%20SDK%20with%20Tracing) [\[12\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=Function%20calling%20allows%20you%20to,between%20your%20applications%20and%20LLMs) [\[13\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=Function%20calling%20is%20useful%20for,of%20use%20cases%2C%20such%20as) [\[14\]](https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api#:~:text=In%20June%202024%2C%20we%20launched,provided%20in%20the%20function%20definition) Function Calling in the OpenAI API \| OpenAI Help Center

<https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api>

[\[17\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=) [\[18\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,Extended%20thinking%20tips) [\[19\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,68) [\[20\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=,70) [\[22\]](https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview#:~:text=1,prompt%20you%20want%20to%20improve) Prompt engineering overview - Claude Docs

<https://docs.claude.com/en/docs/build-with-claude/prompt-engineering/overview>

[\[21\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=We%20recommend%20organizing%20prompts%20into,as%20models%20become%20more%20capable) [\[35\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=After%20a%20few%20years%20of,generate%20our%20model%E2%80%99s%20desired%20behavior) [\[36\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=Context%20engineering%20vs) [\[37\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=In%20the%20early%20days%20of,tools%2C%20%206%20Model%20Context) [\[38\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=generation%20tasks,external%20data%2C%20message%20history%2C%20etc) [\[39\]](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents#:~:text=inference%20and%20longer%20time%20horizons%2C,external%20data%2C%20message%20history%2C%20etc) Effective context engineering for AI agents \\ Anthropic

<https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents>

[\[23\]](https://docs.claude.com/en/api/overview#:~:text=These%20features%20enable%20Claude%20to,line%20operations.Claude%20API) [\[24\]](https://docs.claude.com/en/api/overview#:~:text=MCP%20connector%20Connect%20to%20remote,Claude%20API%20%28Beta) [\[25\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20Web,Claude%20API) [\[26\]](https://docs.claude.com/en/api/overview#:~:text=Tool%20useEnable%20Claude%20to%20interact,Claude%20API) [\[27\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20CitationsGround,Claude%20API) [\[28\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20Search,Claude%20API) [\[29\]](https://docs.claude.com/en/api/overview#:~:text=search%20results%20with%20proper%20source,Claude%20API) [\[40\]](https://docs.claude.com/en/api/overview#:~:text=Google%20Cloud%27s%20Vertex%20AI%20,Claude%20API) Features overview - Claude Docs

<https://docs.claude.com/en/api/overview>

[\[30\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Today%2C%20we%27re%20open,produce%20better%2C%20more%20relevant%20responses) [\[31\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Model%20Context%20Protocol) [\[32\]](https://www.anthropic.com/news/model-context-protocol#:~:text=assistants%20to%20the%20systems%20where,produce%20better%2C%20more%20relevant%20responses) [\[33\]](https://www.anthropic.com/news/model-context-protocol#:~:text=,source%20repository%20of%20MCP%20servers) [\[34\]](https://www.anthropic.com/news/model-context-protocol#:~:text=Claude%203,GitHub%2C%20Git%2C%20Postgres%2C%20and%20Puppeteer) Introducing the Model Context Protocol \\ Anthropic

<https://www.anthropic.com/news/model-context-protocol>

[\[41\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=that%20elicit%20accurate%2C%20high%20quality,responses%20from%20a%20language%20model) [\[43\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=) [\[44\]](https://ai.google.dev/gemini-api/docs/prompting-strategies#:~:text=,Live%20API) Prompt design strategies  \|  Gemini API  \|  Google AI for Developers

<https://ai.google.dev/gemini-api/docs/prompting-strategies>

[\[42\]](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design#:~:text=Gemini%20models%20often%20perform%20well,still%20plays%20an%20important%20role) Introduction to prompting  \|  Generative AI on Vertex AI  \|  Google Cloud Documentation

<https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/introduction-prompt-design>

[\[45\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=The%20Model%20Context%20Protocol%20,more%20accurate%2C%20useful%2C%20and%20automated) [\[46\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=The%20MCP%20creates%20a%20standardized%2C,included%20in%20their%20original%20training) [\[47\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=Large%20language%20models%20,or%20updating%20a%20customer%20record) [\[48\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=At%20its%20core%2C%20the%20Model,email%20it%20to%20my%20manager) [\[49\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=1,the%20company%27s%20database%2C%20and%20retrieves) [\[50\]](https://cloud.google.com/discover/what-is-model-context-protocol#:~:text=existing%20concepts%20like%20tool%20use,included%20in%20their%20original%20training) What is Model Context Protocol (MCP)? A guide \| Google Cloud

<https://cloud.google.com/discover/what-is-model-context-protocol>

[\[51\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=intelligent%2C%20autonomous%20multi,greater%20flexibility%20and%20precise%20control) [\[52\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=Agent%20Development%20Kit%20cloud,greater%20flexibility%20and%20precise%20control) [\[53\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=integration%20letting%20you%20choose%20from,AI21%20Labs%2C%20and%20many%20more) [\[54\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=%2A%20Multi,Enable%20complex%20coordination%20and%20delegation) [\[55\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=,transfer%29%20for%20adaptive%20behavior) [\[56\]](https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/#:~:text=intelligent%2C%20autonomous%20multi,greater%20flexibility%20and%20precise%20control) Agent Development Kit: Making it easy to build multi-agent applications - Google Developers Blog

<https://developers.googleblog.com/en/agent-development-kit-easy-to-build-multi-agent-applications/>
