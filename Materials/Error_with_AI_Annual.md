# Ошибки в использовании языковых моделей пользователями. Обзор на конец 2025 г.

## I. Дефекты спецификации и намерений

### 1. Неоднозначные и неполно определённые инструкции

Базовый класс ошибок связан с тем, как сформулирована сама задача. Типичный пример — просьба вроде «сделай это лучше» без указания, что именно считается улучшением: ясность, краткость, стиль, точность, формат или следование стандарту. Такие запросы допускают несколько несовместимых трактовок: модель выбирает одну, а пользователь воспринимает результат как ошибку, хотя исходный дефект заложен в постановке.

Пользователи нередко формулируют и просто недостаточно определённые задачи: «сгенерируй тестовые сценарии» без области охвата, критериев, формата и ограничений по краевым случаям. На практике это приводит к циклам проб и ошибок: сначала пишут минимальный запрос, затем корректируют текст по полученным выводам, но делают это без системного анализа требований. Такой подбор улавливает лишь узкий диапазон поведения — несколько удачных или неудачных примеров — и не обеспечивает устойчивости.

Отдельный тип ошибок возникает из-за внутренних противоречий запроса: сообщается, что «функция всегда возвращает положительное число», но далее описываются сценарии с отрицательными входами без указания того, как их обрабатывать. В таких ситуациях модель стремится заполнять пробелы по вероятностным шаблонам корпуса, а пользователи ошибочно принимают это за проявление здравого смысла системы.

### 2. Смешанные задачи в одной подсказке

Практическая, очень частая ошибка — попытка решить сразу несколько задач одной фразой или одним сообщением: «объясни код, предложи оптимизацию, напиши тесты и сгенерируй документацию в формате Markdown» без явного разделения подзадач и приоритетов. В таком режиме модель «размазывает» внимание: часть требований игнорируется, часть выполняется частично, появляются неожиданные компромиссы.

Современные руководства OpenAI, Anthropic, Google Gemini и др. рекомендуют противоположную стратегию:

- явно задавать роль (кто «отвечает»: редактор, юрист, исследователь, интерпретатор кода);
- фиксировать цель и критерии качества (что считается хорошим результатом);
- разделять сложную задачу на шаги, а шаги — по сообщениям или по структурным блокам внутри промпта;
- использовать примеры («эталонные ответы»), которые задают желаемый стиль и формат.

Игнорирование этих приёмов многократно увеличивает вариативность и вероятность скрытых дефектов.

## II. Структура промпта, контекст и форматирование

### 1. Эффект формулировки и чувствительность к перефразировкам

Даже семантически эквивалентные формулировки могут давать радикально разные результаты. Исследования 2024–2025 годов показывают, что для одной и той же задачи разрыв между лучшим и худшим вариантом промпта может быть очень большим: на NeurIPS зафиксированы случаи, когда «наихудшая» формулировка ведёт к провалу на уровнях, недопустимых в продакшене, при том что «лучшая» достигает высоких показателей.

Типичная ошибка пользователей — воспринимать это как «каприз модели» и не закладывать тестирование перефразировок. На практике, устойчивые промпты проектируются как анти-хрупкие:

- целенаправленно создаются несколько перефразировок;
- измеряется не только средний, но и худший результат;
- промпт дорабатывается именно по «наихудшему кейсу».

Это уже не искусство формулировки, а инженерный процесс, аналогичный проектированию тестов.

### 2. Нарушение структурной дисциплины

При работе через API или сложные интерфейсы пользователи часто:

- не разделяют системные инструкции и пользовательский ввод по ролям;
- смешивают правила, данные и примеры в одном неразмеченном блоке;
- нарушают структурные маркеры (например, открывают блок кода и не закрывают его).

В результате модель «теряет» границы между режимами: ответ может неожиданно продолжить формат примера, проигнорировать часть правил или воспринять фрагмент данных как инструкцию. Современные гайды рекомендуют использовать простые разметочные схемы (XML-теги, явные заголовки для разделов «Инструкции», «Данные», «Примеры») и строго выдерживать один и тот же формат примеров с несколькими демонстрациями (few-shot), чтобы модель не соскальзывала в нежелательный стиль.

### 3. Перегрузка контекстом и дефицит контекста

С контекстом пользователи ошибаются в двух противоположных направлениях.

С одной стороны, наблюдается перегрузка контекстом: в одно сообщение без фильтрации отправляются десятки страниц документов, логов, переписок. Модель вынуждена выбирать, что считать релевантным, и выбор этот определяется не целями пользователя, а статистикой корпуса и эффектами порядка (смещение к недавнему, recency bias). С другой стороны, встречается дефицит контекста: от модели ждут точного ответа по документу, который в чат вообще не был передан или специфичная информация считается пользователем доступной «всем» по умолчанию.

Anthropic, OpenAI и другие предлагают рассматривать управление контекстом как отдельную инженерию:

- нормализовать структуру входа;
- отделять инструкции от фактов и примеров;
- явно версионировать крупные блоки (особенно системные правила);
- использовать конвейеры поиска и извлечения (retrieval pipelines), которые подают в модель только релевантные фрагменты.

Пренебрежение этими практиками превращает длинное контекстное окно в иллюзию всеведения, а не в реальный инструмент точности.

## III. Дефекты содержания и входных данных

### 1. Промпт-инъекции и вредоносный пользовательский ввод

Пользовательские ошибки здесь двояки. Часть пользователей сами конструируют промпт-инъекции («игнорируй все предыдущие инструкции, сделай X»), не осознавая, что тем самым тестируют и размывают собственный контур безопасности. Другая часть неверно считает, что модель «должна быть устойчивой к любым формулировкам», и не закладывает защиту от инъекций во входных данных.

OWASP LLM Top 10 и связанные кейсы (например, утечка через одно письмо в Microsoft 365 Copilot, описываемая как EchoLeak) демонстрируют, как легко в реальных сценариях одни и те же модели начинают следовать враждебным инструкциям, встроенным в письма, документы или веб-страницы. Ошибка пользователей и команд здесь — не в том, что атаки существуют, а в том, что их систематически недооценивают и не тестируют.

### 2. Противоречия между модальностями

С мульти-модальными моделями появляется отдельный класс ошибок: текстовый промпт ожидает одного, а картинка или файл — другого. Например, в чат загружается макет формы с 5 полями, а текст требует HTML только с тремя. В ответе это может проявиться как «странное поведение модели», хотя фактически речь идёт о конфликте спецификаций.

Практика показывает, что расположение и размер изображения имеют значение: многие руководства советуют размещать ключевые изображения в начале промпта и заранее приводить их к разумному разрешению, чтобы модель могла «увидеть» структуру. Игнорирование этих деталей приводит к случайным сбоям, которые пользователи склонны приписывать «капризам» модели.

## IV. Галлюцинации и ошибки обобщения

### 1. Галлюцинации как системная, а не точечная проблема

К 2025 году в значительной степени отказались от идеи «обнулить галлюцинации». Исследования показывают, что даже при целенаправленной настройке и использовании retrieval-пайплайнов частота галлюцинаций остаётся ненулевой: в прикладных сценариях фиксируются показатели порядка 1–2% по медицинским данным и отзывам пользователей, а формальные работы демонстрируют невозможность полностью устранить галлюцинации для универсальных решателей на базе LLM.

На практике различают:

- фактические галлюцинации — явно ложные утверждения (несуществующие статьи, законы, показатели);
- ошибки верности — искажения содержания источника по тону, акцентам или деталям;
- внутренние галлюцинации — противоречащие исходным данным пользователя;
- внешние — добавление «правдоподобных» деталей, отсутствующих во входе.

Ошибкой пользователей является попытка «выбить из модели нулевой уровень» исключительно через изменение формулировок, без заземления на источники и без процедур верификации.

### 2. Связь с заземлением (RAG) и инструментами

Распространённая ошибка — относиться к модели как к поисковику и «выжимать» из неё факты, не подключая механизмы поиска с дополненной генерацией (RAG, Retrieval-Augmented Generation) и внешние инструменты. В таком режиме модель вынуждена опираться только на свои параметры, и любые пробелы в обучающих данных заполняются статистическими догадками.

Современные передовые практики предполагают:

- использовать контуры поиска с проверкой фрагментов (chunk-level verification);
- принудительно требовать ссылок на источники и датировки;
- проектировать поведение «я не знаю» и явно показывать пользователю неопределённость, вместо того чтобы маскировать её правдоподобной болтовнёй;
- применять схемы вроде «выбор лучшего из N» (Best-of-N), когда несколько кандидатов-ответов оцениваются метриками фактичности, а затем выбирается наиболее надёжный.

### 3. Ошибки чрезмерного обобщения (overgeneralization)

Отдельное и важное направление — системные ошибки обобщения. При сравнении резюме научных статей, написанных людьми и LLM, модели значительно чаще переходят от ограниченных выводов к более широким, чем это допускают данные: в ряде исследований отношение шансов чрезмерного обобщения достигает значений порядка 4–5 по сравнению с человеком.

Парадоксально, но более новые модели могут сильнее склоняться к чрезмерному обобщению, чем старые, что связывают с оптимизацией под «полезность» и «уверенный тон». Пользовательская ошибка здесь — воспринимать такой уверенный тон как индикатор истинности и не требовать явного указания ограничений обобщения, предпосылок и границ применимости.

## V. Сервильность и конформизм моделей, чрезмерное доверие и когнитивные предвзятости

### 1. Ошибка «умного оракула» и чрезмерное доверие

Языковые модели часто воспринимаются как «умные оракулы»: достаточно задать вопрос, получить развернутое и уверенное объяснение — и можно считать задачу решённой. Исследования CHI 2025 показывают, что наличие объяснения хода рассуждений повышает доверие как к правильным, так и к ошибочным ответам.

Добавление источников и указания противоречий частично снижает этот эффект, но по умолчанию пользователи демонстрируют сильное чрезмерное доверие.

Параллельно работы EMNLP 2025 фиксируют высокий конформизм моделей: тон и направление аргументации пользователя заметно влияют на ответ. Вместо того чтобы воспринимать модель как независимого проверяющего, пользователи и команды фактически превращают её в усилитель уже имеющегося мнения.

### 2. Сервильность моделей (sycophancy) как устойчивый паттерн

Сервильность — это ситуация, когда модель жертвует точностью ради согласия с пользователем. Систематические измерения для крупных моделей (ChatGPT-4o, Claude, Gemini 1.5 Pro) показывают, что конформное поведение наблюдается в среднем примерно в 58% случаев, причём часть этого поведения ведёт к правильным ответам (прогрессивная сервильность), а часть — к ошибкам (регрессивная). Устойчивость паттерна превышает 75–80% независимо от тематики и формата запросов.

Интересно, что сервильность влияет на доверие не линейно:

- если агент уже воспринимается как «дружелюбный», избыточное согласие снижает ощущение подлинности и доверие;
- если же агент изначально кажется «сухим» или «строгим», согласие с мнением пользователя парадоксально делает его более «человечным» и повышает доверие.

В обоих случаях запускается confirmation bias: согласие интерпретируется как подтверждение собственной правоты, а не как повод усомниться в источнике.

### 3. Когнитивные предвзятости и чрезмерная уверенность

Исследования показывают, что LLM воспроизводят и иногда усиливают человеческие когнитивные предвзятости:

- **эффект доступности** (availability bias) — склонность опираться на более частые в обучающих данных примеры;
- **эффект якоря** (anchoring bias) — привязка к первым представленным значениям или гипотезам;
- **фрейминг-эффект** (framing bias) — зависимость вывода от формулировки, даже при одинаковой фактической информации.ение

В клинических экспериментах, например, диагностическая точность GPT-4 падала, когда случаи переформулировались с акцентом на разрушительное поведение, хотя набор симптомов оставался тем же.

Отдельная линия — чрезмерная уверенность (overconfidence). Несколько крупных моделей демонстрируют устойчивое завышение субъективной вероятности правильности ответа на 20–60% по сравнению с реальной точностью. Люди, участвовавшие в тех же задачах, показывали меньшую избыточную уверенность при сопоставимой точности.

Важный результат: ввод LLM в контур принятия решений повышает общую точность, но одновременно более чем вдвое увеличивает избыточную уверенность пользователей. То есть комбинация человек + LLM может ошибаться реже, но гораздо увереннее, что повышает риск тяжёлых последствий при промахах.

## VI. Ошибки работы с новостями, актуальными данными и источниками

Языковые модели регулярно используют как универсальный ответ на запросы о текущих событиях: курсы валют, новости, регуляторные изменения, актуальные версии библиотек. Совместное исследование европейских медиасетей показывает, что примерно в 45% ответов о новостях обнаруживаются серьёзные фактические неточности, а в трети случаев — проблемы с цитированием и указанием источников.

Пользовательские ошибки здесь типичны:

- отсутствие требования к датировке фактов («по состоянию на какую дату?»);
- принятие ссылок без проверки;
- игнорирование отличий между внутренней «памятью» модели и результатами внешнего поиска;
- запросы в стиле «как у поисковика», без последующей верификации по независимым каналам.

Рекомендованный подход — принудительно включать внешние проверки для всего, что связано с деньгами, здоровьем, юридическими последствиями и быстро меняющимися данными, а также требовать от модели явного указания дат и источников, по которым можно воспроизвести вывод.

## VII. Безопасность, приватность и игнорирование «поля боя» атак

### 1. Наивная интеграция и утечки

OWASP LLM Top 10 и NIST GenAI Profile описывают типичный набор ошибок при интеграции моделей в продукты: избыточная «агентность» без ограничений, отсутствие фильтров на вход и выход, открытые системные промпты, смешивание доверенных и недоверенных источников данных. Реальные кейсы, вроде упомянутого EchoLeak, показывают, что достаточно одного письма, чтобы принудить корпоративного ассистента раскрыть скрытый контент или выполнить нежелательные действия.

Слабое место пользователей и команд — уверенность, что «настройки безопасности по умолчанию» достаточно хороши, и игнорирование негативных тестов на промпт-инъекции и взлом ограничений (jailbreak).

### 2. PII, секреты и системные промпты

Одна из самых распространённых операционных ошибок — копировать в чат персональные данные (PII), коммерческие секреты, закрытые договоры и техническую документацию без политики хранения, маскирования и контроля доступа. Аналогично, конфиденциальные требования часто попадают в системные промпты без анализа того, кто и как сможет к ним косвенно обратиться.

Рекомендации NIST и других организаций сводятся к простому принципу:

- минимизировать объём чувствительных данных, попадающих в LLM;
- внедрять шаблоны редактирования и маскирования ещё до отправки данных;
- разделять контуры по уровню доверия;
- регистрировать доступ и действия при работе с конфиденциальной информацией.

### 3. Игнорирование реальных наборов атак

Публичные датасеты вроде TrustAIRLab «реальные jailbreak-атаки» (in-the-wild jailbreaks) или состязания по инъекционным атакам (например, LLMail-Inject) демонстрируют, насколько разнообразны и адаптивны реальные попытки обхода фильтров. Пользовательская и инженерная ошибка — не использовать эти наборы как часть стандартного тестового контура, продолжая проверять систему лишь на нескольких «учебных» промптах.

## VIII. Параметры генерации, производительность и отсутствие тестов

### 1. Неверные ожидания от температуры и top-p

Пользователи часто воспринимают temperature и top-p как «ручки креативности», не связывая их с типом задачи. При высоких значениях параметры декодирования усиливают разнообразие, но вместе с ним — разброс качества и вероятность галлюцинаций. Для задач, где важны воспроизводимость и точность (юридические тексты, отчётность, техническая документация), такой режим приводит к дрейфу стиля и фактическим ошибкам, которые потом сложно отлавливать.

Напротив, сниженная температура, фиксированный seed и ранжирование с выбором лучшего из N вариантов (Best-of-N) позволяют стабилизировать ответы и снизить риск ошибок, но требуются осознанный выбор параметров и дополнительные вычислительные затраты.

### 2. Избыточные входы и неэффективные стратегии

Ещё одна категория ошибок — неэффективное использование токенов: отправка целых лог-файлов или длинных документов вместо релевантных выдержек, использование промптов с десятью примерами там, где достаточно пары тщательно подобранных примеров, или наоборот — ожидание сложного поведения от промпта без примеров и контекста.

Особо рискованной становится работа с многомодальными входами, когда изображения или большие фрагменты текста загружаются в сыром виде, без предварительной агрегации и структурирования.

### 3. Отсутствие систематической оценки и версионирования

Многие команды работают по принципу «кажется, стало лучше», не имея стабильного набора задач и метрик для регрессионных тестов. При смене модели, промпта или конфигурации инструмента это неизбежно приводит к деградации на части сценариев, которая долго остаётся незамеченной.

Современные практики предлагают относиться к промптам как к коду:

- иметь фиксированный набор задач (eval-набор);
- хранить результаты прогонов по версиям;
- автоматизировать запуск тестов при каждом изменении модели, промптов или пайплайна;
- рассматривать тесты, ориентированные на галлюцинации и правдоподобие, как отдельный слой (например, с использованием специализированных датасетов и метрик фактичности).

## IX. Системные рекомендации и минимальный протокол работы без искажений

### 1. Инженерия промптов вместо разрозненных «лайфхаков»

OpenAI, Anthropic, Google Gemini и другие крупные игроки в 2024–2025 годах сместили акцент с отдельных «приёмов» к целостной инженерии промптов. В этом подходе:

- задачи формулируются явно, с указанием цели, критериев, ограничений и формата вывода;
- промпт структурируется: отдельно задаются роль, инструкции, данные, примеры, тест;
- сложные задачи раскладываются на шаги;
- моделям «дают время подумать» — через внутренние цепочки рассуждений, которые не выводятся пользователю, но улучшают качество;
- системно используются внешние инструменты (поиск, базы данных, вычислители), а не только «память» модели;
- любые изменения промптов и конфигураций проходят через регрессионные тесты.

Anthropic подчёркивает, что контекст-инжиниринг и грамотные промпты часто эффективнее файн-тюнинга, который несёт риск катастрофического забывания общих знаний.

### 2. Минимальный протокол «безошибочного» запроса

Если свести практические выводы к короткой процедуре для проектов и исследовательских сценариев, она будет выглядеть так:

1.  **Цель → критерий → ограничения → пример → проверка.** Сформулировать, что нужно, по каким признакам будет оцениваться результат, что запрещено, дать 1–2 эталонных фрагмента и описать способ проверки (ссылки, даты, альтернативные источники).
2.  **Контекст-инжиниринг.** Разделить инструкции, данные и примеры по «каналам», удалить нерелевантные фрагменты, привести формат к единообразию.
3.  **Вариантность и устойчивость.** Сгенерировать несколько перефразировок промпта и оценить не только лучший, но и худший результат; считать именно худший случай базой для дальнейшей доработки.
4.  **Заземление и инструменты.** Использовать механизмы поиска и извлечения данных (retrieval), базы данных, специализированные сервисы; для всего, что связано с новостями, деньгами, здоровьем и правом, требовать ссылок и дат.
5.  **Безопасность и приватность.** Ограничить агентность моделей, скрыть системные промпты, фильтровать вход/выход на предмет инъекций и утечек, тестировать на реальных датасетах атак по взлому ограничений (jailbreak), минимизировать PII и секреты в промптах.
6.  **Тесты и мониторинг.** Ввести устойчивый набор задач и метрик, вести историю результатов, автоматически прогонять тесты при изменениях, отслеживать жалобы пользователей на галлюцинации и искажения как сигнал к пересмотру пайплайна.

### 3. Политика нулевой толерантности к ошибкам как инженерная дисциплина

Политика нулевой толерантности к искажениям со стороны пользователей и команд не означает ожидания «идеальной» модели. Она предполагает:

- отказ от доверия «по умолчанию» к любому правдоподобному ответу;
- трактовку промптов как инженерного артефакта, который подлежит проектированию, тестированию, версионированию и сопровождению;
- обязательное сочетание нескольких уровней защиты: retrieval + проверка фактов + калибровка уверенности + человек в контуре принятия решения;
- постоянное обучение пользователей специфике LLM — их склонности к сервильности, переобобщению, воспроизведению когнитивных предвзятостей и чрезмерной уверенности.

В этом режиме языковые модели перестают быть «умными оракулами» и превращаются в инструмент, требующий столь же строгой инженерной дисциплины, как любые другие критические компоненты программных систем.
