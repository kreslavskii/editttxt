# Ошибки в использовании языковых моделей пользователями 1

### 1) Ошибка «умного оракула»: пере-доверие и согласовательность

Пользователи принимают правдоподобные ответы за истинные, особенно когда модель «объясняет» ход мыслей или соглашается с исходным мнением (sycophancy). Эксперименты CHI ’25 показывают рост доверия и к верным, и к ошибочным ответам при наличии объяснений; добавление источников и выявляемых противоречий снижает пере-надежду. EMNLP ’25 фиксирует аргументно-индуцированную «податливость» ответов. Профиль NIST для GenAI относит пере-доверие и автоматизационное смещение к ключевым человеческим рискам. ([ACM Digital Library][1])

### 2) Недоопределённые, смешанные задачи в одной подсказке

Наиболее частая прикладная ошибка — расплывчатые цели, не заданные критерии качества/формата, смешение нескольких задач. Официальные руководства OpenAI/Gemini/Anthropic прямо рекомендуют явные роли, ограничения, примеры, пошаговое разложение и структурирование контекста; игнорирование этих приёмов резко ухудшает устойчивость. ([Платформа OpenAI][2])

### 3) Игнорирование чувствительности к формулировке

Даже семантически эквивалентные формулировки дают сильно разные результаты; отказ тестировать варианты — систематическая ошибка. NeurIPS ’24 демонстрирует «наихудшую производительность» на безобидных перефразировках, определяя широкий разрыв между лучшим и худшим кейсами для одной и той же задачи. Практический вывод — A/B-варианты и «анти-хрупкие» промпты. ([Proceedings NeurIPS][3])

### 4) Отсутствие «заземления» и инструментов

Пытаться «выжать факты из модели» без RAG/инструментов — источник галлюцинаций. И гайды OpenAI/Anthropic, и NIST призывают подключать поиск/БД, требовать проверяемые источники и проектировать явные проверки; игнорирование этого — повторяющаяся причина ошибок в продакшене. ([Платформа OpenAI][2])

### 5) Плохая работа с контекстом

Два полюса: «контекстный бродкаст» (слишком много нерелевантного) и «контекстный голод» (недостаток опор). Anthropic предлагает контекст-инжиниринг: нормализовать структуру, отделять требования от данных, использовать «каналы» (инструкции/факты/примеры) и версии. Пренебрежение этими приёмами ведёт к сбоему управлению долгими окнами. ([Anthropic][4])

### 6) Небезопасная интеграция и наивность к атакам

Пользователи и команды часто: (а) не учитывают prompt-injection/indirect injection, (б) не ограничивают «агентность» ассистента, (в) допускают утечки системных промптов и секретов. OWASP LLM Top 10 (2025) фиксирует эти риски и паттерны защиты; реальный «нуль-клик» кейс EchoLeak (Microsoft 365 Copilot) демонстрирует утечку через одно письмо. ([OWASP Gen AI Security Project][5])

### 7) Непроверенные ответы по новостям/актуалиям

Стабильная ошибка — спрашивать модели «как у поисковика» и не проверять цитирование. Совместное исследование EBU/BBC (2025) на 3000 ответах: у ~45 % серьёзные ошибки, у 33 % — проблемы с источниками. Вывод: требовать явные ссылки, датировать факты, принудительно запускать внешнюю проверку. ([Reuters][6])

### 8) Неверные ожидания от параметров выборки

Высокая температура/топ-p без осмысленного T-шутинга ради «креативности» — частая причина дрейфа и несогласованности. Официальные гайды прямо увязывают параметры с задачей: детерминированность/редакция vs. дивергентная генерация. ([Платформа OpenAI][2])

### 9) Отсутствие систематической оценки

Работа «по ощущениям», без регрессионных эвальюаций, — причина деградации при смене моделей/версий/инструкций. Рекомендованы task-специфичные эвалы (OpenAI Evals), а для правдоподобия/галлюцинаций — современные бенчмарки (напр., HalluMix). ([GitHub][7])

### 10) Концептуальная подмена: «понимание» ↔ предсказание токенов

Сильная и устойчивая ошибка интерпретации — приписывать моделям человеческое понимание/суждение. Рецензируемая работа в SAGE (2025) формулирует типовые софизмы подстановки LLM вместо человеческого рассуждения; практическая мера — проектировать проверки на правдоподобие vs. истину. ([SAGE Journals][8])

### 11) Невнимание к приватности и данным

Копирование PII/секретов в чат, отсутствие политики retention/шаблонов редактирования — одна из самых частых операционных ошибок; это же относится к некорректному переносy конфиденциальных требований в системный промпт. Профили NIST (2024–25) дают конкретные управленческие меры. ([NIST Публикации][9])

### 12) Игнорирование эмпирики атак и «живых» датасетов

Команды недооценивают масштаб и разнообразие jailbreak-шаблонов и тем самым «обучают» модели/агентов на уязвимых паттернах. Публичные наборы (TrustAIRLab «in-the-wild jailbreaks»; Microsoft LLMail-Inject challenge) демонстрируют, как легко обходятся наивные фильтры. ([Hugging Face][10])

## Мини-шпаргалка «ошибка → симптом → что делать»

| Ошибка                             | Симптом                                       | Мера                                                                                                                                                           |
| ---------------------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Пере-доверие/согласовательность    | Уверенные, но неверные ответы; «поддакивание» | Требовать источники; встраивать несогласие/контр-примеры; обучать «эпистемической скромности» UI-сигналами. ([ACM Digital Library][1])                         |
| Недоопределённый промпт            | Размытый стиль, несоответствие формату        | Явная роль, цель, критерии, примеры; отделить инструкцию/данные/пример. ([Платформа OpenAI][2])                                                                |
| Неустойчивость к перефразировкам   | «Флуктуации» качества от мелких правок        | Тестировать варианты; фиксировать «наихудший кейс»; писать анти-хрупкие подсказки. ([Proceedings NeurIPS][3])                                                  |
| Отсутствие RAG/инструментов        | Галлюцинации, устаревание фактов              | Подключать поиск/БД/функции; требовать датировки/ссылок. ([Платформа OpenAI][2])                                                                               |
| Плохой контекст-менеджмент         | «Захламлённые» или «голодные» запросы         | Контекст-инжиниринг: каналы, дедуп, версионирование. ([Anthropic][4])                                                                                          |
| Наивная безопасность               | Выполнение враждебных инструкций; утечки      | OWASP LLM Top 10 → разграничение агентности, санитизация ввода/вывода, защита системного промпта, «невидимость» секретов. ([OWASP Gen AI Security Project][5]) |
| Вопросы про новости «как в поиске» | Неверные/устаревшие «факты»                   | Принудительная верификация источников, явная дата-логика. ([Reuters][6])                                                                                       |
| Параметры по умолчанию             | Случайность/дрейф стиля                       | Подбирать temperature/top-p под задачу; фиксировать seed/вариант. ([Платформа OpenAI][2])                                                                      |
| Нет регрессионных эвальюаций       | Деградация после апдейтов                     | Завести стабильные эвалы/данные, хранить метрики, автоматизировать «прохождение ворот». ([GitHub][7])                                                          |
| PII/секреты в промптах             | Правовые/репутационные риски                  | Политики редактирования/маскирования; изоляция контуров; журналирование доступа. ([NIST Публикации][9])                                                        |
| Игнорирование «поля боя» атак      | Повторные прорывы фильтров                    | Тренировать/тестировать на реальных наборах jailbreak/injection; проводить красное командование. ([Hugging Face][10])                                          |

1. Исследования CHI/EMNLP фиксируют не частные «галлюцинации», а системный человеческо-машинный контур: объяснения и диалог усиливают пере-доверие; согласовательность модели искажается формулировкой аргумента, значит, дизайн UX и методика вопроса — такие же важные рычаги, как архитектура модели. ([ACM Digital Library][1])
2. Появился консенсус стандартов/безопасности: NIST GenAI Profile + OWASP LLM Top 10 описывают шаблоны именно пользовательских/интеграционных ошибок (избыточная агентность, неправильная обработка вывода, утечки системного промпта, непродуманные доверия к внешнему контенту). ([NIST Публикации][9])
3. Официальные гайды (OpenAI/Gemini/Anthropic/xAI) сместились от «лайфхаков» к процессу: контекст-инжиниринг, разделение ролей/данных, заземление на инструменты, протоколирование и эвальюации как норма. ([Платформа OpenAI][2])
4. Эмпирика атак и датасеты HuggingFace/GitHub демонстрируют, что «редкие» джейлбрейки — массовый материал, и игнорировать его в разработке/тестировании — практическая халатность. ([Hugging Face][10])

## Мини-процедура безошибочного запроса (применимо в продакшене и исследовании)

1. Цель-критерий-ограничения → примеры → проверка: сформулировать задачу, формат и запретные зоны; добавить 1–2 эталонных примера; описать проверку (линки, даты, тест). ([Платформа OpenAI][2])
2. Контекст-инжиниринг: вынести системные правила, факты, примеры в отдельные «каналы», отфильтровать нерелевант. ([Anthropic][4])
3. Вариантность: запуск 3–5 перефразировок и выбор «наихудшего» как регрессионного теста. ([Proceedings NeurIPS][3])
4. Заземление: принудительный RAG/инструменты и требование датированных источников. ([NIST Публикации][9])
5. Безопасность: фильтры на вход/выход, ограничения агентности, защита системного промпта; негативные тесты на injection/jailbreak из публичных наборов. ([OWASP Gen AI Security Project][5])
6. Эвалы: фиксированный набор задач/метрик, автоматический прогон при изменениях модели/промптов. ([GitHub][7])

**Ключевые опорные источники (по приоритету и новизне):**
CHI ’25 о надлежащем доверии; EMNLP ’25 о согласовательности; NIST AI 600-1 Generative AI Profile (2024/25); OWASP LLM Top 10 (2025); официальные промпт-гайды OpenAI/Gemini/Anthropic. ([ACM Digital Library][1])

[1]: https://dl.acm.org/doi/10.1145/3706598.3714020?utm_source=chatgpt.com "Fostering Appropriate Reliance on Large Language Models"
[2]: https://platform.openai.com/docs/guides/prompt-engineering?utm_source=chatgpt.com "docs/guides/prompt-engineering"
[3]: https://proceedings.neurips.cc/paper_files/paper/2024/file/7fa5a377b7ffabcce43cd00231bb3f9c-Paper-Conference.pdf?utm_source=chatgpt.com "On the Worst Prompt Performance of Large Language ..."
[4]: https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents?utm_source=chatgpt.com "Effective context engineering for AI agents"
[5]: https://genai.owasp.org/llm-top-10/?utm_source=chatgpt.com "LLMRisks Archive - OWASP Gen AI Security Project"
[6]: https://www.reuters.com/business/media-telecom/ai-assistants-make-widespread-errors-about-news-new-research-shows-2025-10-21/?utm_source=chatgpt.com "AI assistants make widespread errors about the news, new research shows"
[7]: https://github.com/openai/evals?utm_source=chatgpt.com "openai/evals"
[8]: https://journals.sagepub.com/doi/10.1177/25152459251357566?utm_source=chatgpt.com "Six Fallacies in Substituting Large Language Models for ..."
[9]: https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf?utm_source=chatgpt.com "Artificial Intelligence Risk Management Framework"
[10]: https://huggingface.co/datasets/TrustAIRLab/in-the-wild-jailbreak-prompts?utm_source=chatgpt.com "TrustAIRLab/in-the-wild-jailbreak-prompts"


